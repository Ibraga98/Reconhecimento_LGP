# ğŸ¤– Projeto de Reconhecimento de Gestos LGP (LÃ­ngua Gestual Portuguesa)

Este projeto tem como objetivo reconhecer gestos da LGP atravÃ©s de uma webcam, utilizando um modelo de visÃ£o computacional treinado com TensorFlow/Keras e posteriormente convertido para utilizaÃ§Ã£o em uma aplicaÃ§Ã£o Android com Flutter.

---

## ğŸ“¦ Estrutura do Projeto

```
â”œâ”€â”€ captura_gestos.py                # Captura imagens para treino
â”œâ”€â”€ treinar_modelo.py               # Treina modelo CNN com imagens do dataset
â”œâ”€â”€ modelo_gestos_lgp.h5            # Modelo treinado em Keras
â”œâ”€â”€ modelo_gestos_lgp.tflite        # Modelo convertido para uso em Android
â”œâ”€â”€ classes.json                    # Mapeamento de classes (gestos)
â”œâ”€â”€ inferencia_tempo_real.py       # Testes em tempo real com webcam
â”œâ”€â”€ detecao_maos_mediapipe.py      # MÃ³dulo de deteÃ§Ã£o de mÃ£os com MediaPipe
â”œâ”€â”€ testar_modelo.py               # Verifica a performance do modelo treinado
â”œâ”€â”€ dataset_limpo/                 # Dataset organizado por classes
â”œâ”€â”€ modelo_android/                # Pasta especÃ­fica para integraÃ§Ã£o com Android
â”‚   â”œâ”€â”€ modelo_gestos_lgp.tflite
â”‚   â”œâ”€â”€ classes.json
â”‚   â”œâ”€â”€ instrucoes.txt
â”‚   â””â”€â”€ exemplo_flutter_tflite.dart
```

---

## ğŸ§  Modelo

- Modelo CNN simples com `Conv2D`, `MaxPooling2D`, `Flatten` e `Dense`
- Treinado com imagens 100x100px RGB normalizadas
- Exportado para `.tflite` com `TFLiteConverter`

---

## ğŸ–¥ï¸ ExecuÃ§Ã£o no PC

```bash
python captura_gestos.py           # Captura imagens
python treinar_modelo.py          # Treina o modelo (necessÃ¡rio apÃ³s capturar novas imagens)
python converter_para_tflite.py   # Converte o modelo para .tflite
python inferencia_tempo_real.py   # Testa em tempo real
```

### âš ï¸ Fluxo de Trabalho Importante

1. Sempre que novas imagens forem capturadas com `captura_gestos.py`, Ã© **necessÃ¡rio** executar `treinar_modelo.py` novamente para que o modelo aprenda com as novas imagens.
2. O modelo treinado (`modelo_gestos_lgp.h5`) sÃ³ reconhecerÃ¡ gestos que estavam presentes no dataset durante o Ãºltimo treinamento.
3. ApÃ³s treinar o modelo, vocÃª pode testÃ¡-lo com `inferencia_tempo_real.py` ou `testar_modelo.py`.

---

## ğŸ“± IntegraÃ§Ã£o Android

A pasta `modelo_android/` contÃ©m:

- `modelo_gestos_lgp.tflite`
- `classes.json`
- `instrucoes.txt`
- `exemplo_flutter_tflite.dart`

Utilizar `tflite_flutter` no Flutter para carregar e usar o modelo.

---

## ğŸ‘¨â€ğŸ’» Autores

- ğŸ‘¤ Ivanilson Braga â€” ImplementaÃ§Ã£o em Python, treino do modelo, deteÃ§Ã£o de mÃ£os
- ğŸ‘¤ Zakhar Khomyakivskyy IntegraÃ§Ã£o com Flutter e desenvolvimento Android

---

## âœ… Estado Atual

âœ”ï¸ Dataset personalizado criado  
âœ”ï¸ Modelo treinado com alta precisÃ£o  
âœ”ï¸ DeteÃ§Ã£o de mÃ£os em tempo real integrada  
âœ”ï¸ CÃ³digo limpo e documentado  
â³ Pronto para integraÃ§Ã£o no Android
