ğŸ¤Ÿ Sistema de Reconhecimento de Gestos em LÃ­ngua Gestual Portuguesa (LGP)

Este repositÃ³rio apresenta um sistema desenvolvido para o reconhecimento de gestos em **LÃ­ngua Gestual Portuguesa (LGP)**, com o objetivo de promover a inclusÃ£o social de pessoas surdas, facilitando a comunicaÃ§Ã£o com ouvintes atravÃ©s da traduÃ§Ã£o de gestos em tempo real para texto.

O projeto foi desenvolvido por **Ivanilson Braga**, **Zakhar Khomyakivskyy** e **Ektiandro Elizabeth**, no Ã¢mbito da unidade curricular de Laboratorio De Projeto.



ğŸ“‹ DescriÃ§Ã£o Geral

A aplicaÃ§Ã£o permite a deteÃ§Ã£o de gestos capturados por webcam, utilizando visÃ£o computacional e redes neurais convolucionais (CNN), e converte esses gestos em palavras escritas. A longo prazo, o sistema pode ser integrado a dispositivos mÃ³veis, aumentando o seu impacto social e acessibilidade.



ğŸ“Œ Palavras Atualmente Reconhecidas

O sistema foi treinado para reconhecer os seguintes gestos:

- **OlÃ¡**
- **Sim**
- **NÃ£o**
- **Ãgua**
- **Por favor**
- **Bom dia**
  
Mais palavras podem ser adicionadas com facilidade atravÃ©s da captura de novos gestos e re-treinamento do modelo.



ğŸ›  Tecnologias Utilizadas

- **Python 3.10+** â€“ Linguagem principal de desenvolvimento
- **TensorFlow / Keras** â€“ ConstruÃ§Ã£o e treino da rede neural
- **MediaPipe** â€“ DeteÃ§Ã£o de mÃ£os em tempo real
- **OpenCV** â€“ Captura e visualizaÃ§Ã£o da webcam
- **Flutter** â€“ Interface mÃ³vel Android (em desenvolvimento)
- **TensorFlow Lite** â€“ VersÃ£o leve do modelo para dispositivos mÃ³veis



ğŸ“‚ Estrutura do Projeto

```bash
ğŸ“ Reconhecimento_LGP/
â”‚
â”œâ”€â”€ captura_gestos.py            # Captura imagens da webcam (sÃ³ quando a mÃ£o Ã© detetada)
â”œâ”€â”€ limpar_dataset_externo.py    # Elimina imagens sem mÃ£os
â”œâ”€â”€ treinar_modelo.py            # Treina o modelo (.h5) com base nas imagens captadas
â”œâ”€â”€ converter_para_tflite.py     # Converte o modelo para formato .tflite (mobile)
â”œâ”€â”€ testar_modelo.py             # Testa a precisÃ£o do modelo com webcam
â”œâ”€â”€ inferencia_tempo_real.py     # Reconhece gestos Ãºnicos em tempo real
â”œâ”€â”€ inferencia_com_frase.py      # Reconhece frases com mÃºltiplos gestos e pausas
â”‚
â”œâ”€â”€ modelo_gestos_lgp.h5         # Modelo treinado (Keras)
â”œâ”€â”€ modelo_gestos_lgp.tflite     # Modelo convertido para Android
â”œâ”€â”€ classes.json                 # Lista de classes/gestos reconhecidos
â”œâ”€â”€ README.md                    # Ficheiro atual
â””â”€â”€ dataset_limpo/               # Dataset final utilizado no treino, com imagens limpas
```



 ğŸš€ Como Usar

 1. Capturar imagens da webcam
Executa o script e digita o nome do gesto (ex: "sim"):
```bash
python captura_gestos.py
```

 2. Limpar imagens sem mÃ£os (automÃ¡tico com MediaPipe)
```bash
python limpar_dataset_externo.py
```

3. Treinar o modelo
```bash
python treinar_modelo.py
```

 4. Converter para TensorFlow Lite (.tflite)
```bash
python converter_para_tflite.py
```

 5. Testar o modelo com webcam
```bash
python testar_modelo.py
```

 6. Reconhecimento de frases com pausa
```bash
python inferencia_com_frase.py
```

---
ğŸ¤– AplicaÃ§Ã£o Android (em desenvolvimento)

A versÃ£o mÃ³vel da aplicaÃ§Ã£o estÃ¡ a ser desenvolvida em **Flutter**, e utilizarÃ¡ o ficheiro `modelo_gestos_lgp.tflite` juntamente com `classes.json` para realizar inferÃªncia local no smartphone, sem necessidade de ligaÃ§Ã£o Ã  internet.

O processamento serÃ¡ todo feito no dispositivo, usando a cÃ¢mara para interpretar os gestos em tempo real com baixa latÃªncia.



## ğŸ§  Como o modelo funciona?

O modelo baseia-se numa **CNN (Convolutional Neural Network)**, treinada com imagens captadas via webcam, que passaram por um processo de limpeza e normalizaÃ§Ã£o. Utiliza `categorical_crossentropy` como funÃ§Ã£o de perda e `adam` como otimizador.

A entrada do modelo sÃ£o imagens de 100x100 pixels, normalizadas. A saÃ­da Ã© uma prediÃ§Ã£o entre as classes configuradas em `classes.json`.



ğŸ‘¥ Contribuidores

- **Ivanilson Braga**  
- **Zakhar Khomyakivskyy**  
- **Ektiandro Elizabeth**


ğŸ§¾ LicenÃ§a

Projeto desenvolvido para fins acadÃ©micos no Ã¢mbito da unidade curricular de Projeto.  
Qualquer reutilizaÃ§Ã£o parcial ou total deve referenciar os autores.


ğŸ’¡ ConsideraÃ§Ãµes Finais

Este projeto mostrou-se eficaz na deteÃ§Ã£o e traduÃ§Ã£o de gestos da LGP para texto, abrindo portas para futuras expansÃµes com mais sinais, integraÃ§Ã£o por voz, suporte para outras lÃ­nguas gestuais e interfaces multimodais. A integraÃ§Ã£o com Flutter e a utilizaÃ§Ã£o do TensorFlow Lite tornam este sistema promissor para aplicaÃ§Ã£o prÃ¡tica no quotidiano.

